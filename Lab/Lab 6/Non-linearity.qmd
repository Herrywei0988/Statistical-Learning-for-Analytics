---
format:
  html:
    embed-resources: true
    toc: true
title: 'Lab: Non-linearity'
---



**Instructions**

* Read and complete all exercises below in the provided `.qmd` notebook (or `.ipynb` file if you prefer, both files are identical aside from the formatting)

**Submission:**

* You need to upload ONE document to Canvas when you are done. 
* A PDF (or HTML) of the completed form of this notebook
* The final uploaded version should NOT have any code-errors present. 
* All outputs must be visible in the uploaded version, including code-cell outputs, images, graphs, etc


**Optional**: 

* You can actually work in `R` now inside VS-code using the `.ipynb` format
* Its quite easy to get working: [click here for instructions](https://www.practicaldatascience.org/html/jupyter_r_notebooks.html)
* There are a few benefits to this 
  * (1) You can work 100% in VS-Code (for both R and Python), no need to switch between VSC and R-studio
  * (2) You can work through your cells one at a time and see the incremental progress, similar to using `.ipynb` with python or `rmd` in R-studio.
* With Quarto's `convert` command, you can re-format and jump between the different file-formats. For example, 
* `quarto convert HW-2.rmd` will convert the file to `HW-2.ipynb`
* `quarto convert HW-2.ipynb` will convert the file to `HW-2.qmd`, which can be renamed `HW-2.rmd` or just opened in R-studio like any other `rmd` file, just like normal.
* `quarto render HW-2.ipynb` will render the notebook (either R or Python) into an aesthetically pleasing output.

```{r}
# install.packages("dslabs")
```

```{r}
#| vscode: {languageId: r}
# IMPORT LIBRARIES 
library(ISLR)
library(gam)
library(splines)
library(tidyverse)
library(caret)
```

# Part-1: Demonstration

## Polynomial Regression

### Example 1

Make a sequence of x values to build model matrices.
Also make a nonlinear response variable and store everything in a data frame.

```{r}
#| vscode: {languageId: r}
# GENERATE SOME DATA AND PLOT
x = seq(-3,3,by=.01) #random numbers
y = cos(x) + rnorm(length(x))/2 #non-linear function
mydf = data.frame(x = x, y = y)
print(dim(mydf))
plot(mydf)
```

Make the model matrix for simple linear regression.

```{r}
#| vscode: {languageId: r}
# ADD UNIT FEATURE 
X1 = model.matrix(y ~ x) #create a design matrix
print(head(X1))
```

Make a model matrix for polynomial regression.
The basis functions are the powers of the X variable.

```{r}
#| vscode: {languageId: r}
# ADD UNIT FEATURE AND TRANSFORM
X6 = model.matrix(y ~ x + I(x^2) + I(x^3) + I(x^4) + I(x^5) + I(x^6))
print(head(X6))
```

Also make a similar model matrix using the poly() function.
This also makes basis functions which are polynomials of increasing degree.
However, these basis functions have better numerical and statistical properties as we will see shortly.

```{r}
#| vscode: {languageId: r}
X6p = model.matrix(y ~ poly(x,6))
print(head(X6))
print(head(X6p)) #values are different
```

Make a plot of the basis functions x, x^2, ..., x^6.
Since these values are fairly large near the ends of the interval, we need a large ylim.

```{r}
#| vscode: {languageId: r}
plot(x,X6[,1], type = 'l', ylim = c(-50,50))
matlines(x,X6[,2:7],lwd = 2)
grid(col = 3) #plotting the polynomials with degree 1,..,6
```

Compute also the correlation coefficients.
As you can see, even powers are correlated with each other and odd powers are also correlated with each other.

```{r}
#| vscode: {languageId: r}
print(round(cor(X6[,2:7]),2)) 
#what can you say about the correlation?high?low? misfitting? if the features are correlated then what?==> multicollinearity ==>lead to numerical instability of fitting the model
```

By contrast, the basis function generated by poly() are not correlated at all.
They are "orthogonal polynomials".

```{r}
#| vscode: {languageId: r}
print(round(cor(X6p[,2:7]),2))
#what do you notice? poly() invokes othogonal polynomials. 
#donot want linear combinations of features that create other features, we want it to be othonormal basis sets ~ linear Algebra
```

The basis functions x, x^2, ... also have less desirable numerical properties.
In the matrix formulation of linear regression, the following matrix and its inverse appear:

```{r}
#| vscode: {languageId: r}
A = t(X6)%*%X6 #X^TX (XtransposeX) %*% - matrix multiplication
B = solve(A) #matrix inverse
```

Then A*B should be the identity matrix.
Let's compute the product and check how this differs from the identity matrix:

```{r}
#| vscode: {languageId: r}
max(abs(A%*%B -diag(rep(1,7)))) #this should be 0 if it is equal to the identity matrix
```

This should really be much smaller. However, for this particular problem, it does not cause trouble.

Now plot the orthogonal basis functions generated by poly().

```{r}
#| vscode: {languageId: r}
plot(x,X6p[,1], type = 'l', ylim = c(-2,2))
matlines(x,X6p[,2:7],lwd = 2)
grid(col = 3)
```

All basis functions are much more similar in magnitude.
Zoom in:

```{r}
#| vscode: {languageId: r}
plot(x,0*x,ylim=c(-.2,.2), type = 'l')
matlines(x,X6p[,2:7],lwd = 2)
grid(col = 3) #6 polynomials
#othornormal basis set ==> much numerical stability for our model
```

These basis functions are  all uncorrelated. That is a consequence (and in fact essentially equivalent) to being orthogonal. 

```{r}
#| vscode: {languageId: r}
print(round(cor(X6p[,2:7]),2))
```

Their numerical properties are also better.

```{r}
#| vscode: {languageId: r}
A = t(X6p)%*%X6p
B = solve(A)
max(abs(A%*%B -diag(rep(1,7)))) #should be close to 0
```

```{r}
#| vscode: {languageId: r}
fit=lm(y ~ poly(x,6))
print(summary(fit)) #what do you notice? Which predictors are worth keeping in the model?remeber what the data looks like? does this makes sense?
```

### Example 1

We’ll use the Boston data set [in MASS package], for predicting the median house value (mdev), in Boston Suburbs, based on the predictor variable lstat (percentage of lower status of the population).

We’ll randomly split the data into training set (80% for building a predictive model) and test set (20% for evaluating the model). Make sure to set seed for reproducibility.

```{r}
#| vscode: {languageId: r}
# Load the data
data("Boston", package = "MASS")
# Split the data into training and test set
set.seed(123)
training.samples <- Boston$medv %>%
  createDataPartition(p = 0.8, list = FALSE)
train.data  <- Boston[training.samples, ]
test.data <- Boston[-training.samples, ]
```

First, visualize the scatter plot of the medv vs lstat variables as follow:

```{r}
#| vscode: {languageId: r}
ggplot(train.data, aes(lstat, medv) ) +
  geom_point() +
  stat_smooth()
```

Let's compare the different models in order to choose the best one for our data.

The standard linear regression model equation can be written as medv = b0 + b1*lstat.

(a). Compute linear regression model:

```{r}
#| vscode: {languageId: r}
# Build the model
model <- lm(medv ~ lstat, data = train.data)
# Make predictions
predictions <- model %>% predict(test.data)
# Model performance
print(data.frame(
  RMSE = RMSE(predictions, test.data$medv),
  R2 = R2(predictions, test.data$medv)
))
```

Visualize the data:

```{r}
#| vscode: {languageId: r}
ggplot(train.data, aes(lstat, medv) ) +
  geom_point() +
  stat_smooth(method = lm, formula = y ~ x)
```

(b). Polynomial regression

The polynomial regression adds polynomial or quadratic terms to the regression equation as follow:

$medv=b0+b1*lstat+b2*lstat^2$

In R, to create a predictor x^2 you should use the function I(), as follow: I(x^2). This raise x to the power 2.

```{r}
#| vscode: {languageId: r}
lm(medv ~ lstat + I(lstat^2), data = train.data)
```

An alternative simple solution is to use this:

```{r}
#| vscode: {languageId: r}
lm(medv ~ poly(lstat, 2, raw = TRUE), data = train.data)
```

The output contains two coefficients associated with lstat : one for the linear term (lstat^1) and one for the quadratic term (lstat^2).

The following example computes a sixfth-order polynomial fit:

```{r}
#| vscode: {languageId: r}
lm(medv ~ poly(lstat, 6, raw = TRUE), data = train.data) %>%
  summary()
```

From the output above, it can be seen that polynomial terms beyond the fith order are not significant. So, just create a fith polynomial regression model as follow:

```{r}
#| vscode: {languageId: r}
# Build the model
model <- lm(medv ~ poly(lstat, 5, raw = TRUE), data = train.data)
# Make predictions
predictions <- model %>% predict(test.data)
# Model performance
print(data.frame(
  RMSE = RMSE(predictions, test.data$medv),
  R2 = R2(predictions, test.data$medv)
))
```

Visualize the fith polynomial regression line as follow:

```{r}
#| vscode: {languageId: r}
ggplot(train.data, aes(lstat, medv) ) +
  geom_point() +
  stat_smooth(method = lm, formula = y ~ poly(x, 5, raw = TRUE))
```

## Cubic Splines and Smoothing Splines

Cubic spline with fixed knots.

```{r}
#| vscode: {languageId: r}
#library(splineDesign)
attach(Wage)
agelims=range(Wage$age)
age.grid=seq(from=agelims[1],to=agelims[2])
spline_fit <- lm(wage ~ bs(age, knots=c(20,40,60)), data=Wage) 

plot(Wage$age, Wage$wage, pch=19, col='grey')
pred = predict(spline_fit,list(age=age.grid), se=T)
lines(age.grid,pred$fit, col="#3690C0",lwd=4)
```

 A natural spline has better behavior at the boundary points.

```{r}
#| vscode: {languageId: r}
agelims=range(Wage$age)
age.grid=seq(from=agelims[1],to=agelims[2])

spline_fit <- lm(wage ~ ns(age, knots=c(20,40,60)), data=Wage)
plot(Wage$age, Wage$wage, pch=19, col='grey')
pred = predict(spline_fit,list(age=age.grid), se=T)
lines(age.grid,pred$fit, col="#3690C0",lwd=4)
```

A smoothing spline is a cubic spline with a knot at every observed $x$ but also a penalization to encourage smoothness.

```{r}
#| vscode: {languageId: r}
plot(Wage$age, Wage$wage, pch=19, col='grey')
ss_fit <- smooth.spline(Wage$age,Wage$wage,cv=TRUE) 
lines(ss_fit,col="firebrick", lwd=4)
```

### Example 2

For our previous example, we’ll place the knots at the lower quartile, the median quartile, and the upper quartile:

```{r}
#| vscode: {languageId: r}
knots <- quantile(train.data$lstat, p = c(0.25, 0.5, 0.75))
```

We’ll create a model using a cubic spline (degree = 3):

```{r}
#| vscode: {languageId: r}
# Build the model
knots <- quantile(train.data$lstat, p = c(0.25, 0.5, 0.75))
model <- lm (medv ~ bs(lstat, knots = knots), data = train.data)
# Make predictions
predictions <- model %>% predict(test.data)
# Model performance
print(data.frame(
  RMSE = RMSE(predictions, test.data$medv),
  R2 = R2(predictions, test.data$medv)
))
```

Note that, the coefficients for a spline term are not interpretable.

Visualize the cubic spline as follow:

```{r}
#| vscode: {languageId: r}
ggplot(train.data, aes(lstat, medv) ) +
  geom_point() +
  stat_smooth(method = lm, formula = y ~ splines::bs(x, df = 3))
```

## GAM 

```{r}
#| vscode: {languageId: r}
# Build the model
model <- gam(medv ~ s(lstat), data = train.data)
# Make predictions
predictions <- model %>% predict(test.data)
# Model performance
print(data.frame(
  RMSE = RMSE(predictions, test.data$medv),
  R2 = R2(predictions, test.data$medv)
))
```

```{r}
#| vscode: {languageId: r}
ggplot(train.data, aes(lstat, medv) ) +
  geom_point() +
  stat_smooth(method = gam, formula = y ~ s(x))
```

From analyzing the RMSE and the R2 metrics of the different models, it can be seen that the polynomial regression, the spline regression and the generalized additive models outperform the linear regression model and the log transformation approaches.

### From the book

```{r}
#| vscode: {languageId: r}
# print(wage~ns(year,4))
```

```{r}
#| vscode: {languageId: r}
print(class(Wage$education))
```

```{r}
#| vscode: {languageId: r}
gam1=lm(wage~ns(year,4)+ns(age,5)+education,data=Wage)
gam.m3=gam(wage~s(year,4)+s(age,5)+education,data=Wage)

summary(gam.m3)

plot(gam.m3, se=TRUE,col="blue")
```

```{r}
#| vscode: {languageId: r}
gam.m1=gam(wage~s(age,5)+education,data=Wage)
gam.m2=gam(wage~year+s(age,5)+education,data=Wage)
gam.m3=gam(wage~s(year,4)+s(age,5)+education,data=Wage)
print(anova(gam.m1,gam.m2,gam.m3,test="F"))

preds=predict(gam.m2,newdata=Wage)

gam.lo=gam(wage~s(year,df=4)+lo(age,span=0.7)+education,data=Wage)
plot(gam.lo, se=TRUE, col="green")
```

## LOESS

```{r}
#| vscode: {languageId: r}
library(tidyverse)
library(dslabs)
library(caret)

data("polls_2008")
qplot(day, margin, data = polls_2008)

total_days <- diff(range(polls_2008$day))
span <- 21/total_days

fit <- loess(margin ~ day, degree=1, span = span, data=polls_2008)

polls_2008 %>% mutate(smooth = fit$fitted) %>%
  ggplot(aes(day, margin)) +
  geom_point(size = 3, alpha = .5, color = "grey") +
  geom_line(aes(day, smooth), color="red")
```

### From the book

```{r}
#| vscode: {languageId: r}
attach(Wage)
plot(age,wage,xlim=agelims ,cex=.5,col="darkgrey")
fit=loess(wage~age,span=.2,data=Wage)
fit2=loess(wage~age,span=.5,data=Wage)
lines(age.grid,predict(fit,data.frame(age=age.grid)),col="red",lwd=2)
lines(age.grid,predict(fit2,data.frame(age=age.grid)),col="blue",lwd=2)
legend("topright",legend=c("Span=0.2","Span=0.5"),col=c("red","blue"),lty=1,lwd=2,cex=.8)
```

# Part-2: Lab-Assignment


## Question-1: 

Please write a paragraph about what you can conclude from this example (i.e. final Conclusions).  

### S&P Data: Discuss this with your group members 

We will working with the weekly S&P500 data.

```{r}
#| vscode: {languageId: r}
print(names(Weekly))
print(head(Weekly))
attach(Weekly) #time series data
```

We want to predict weekly Volume from performance in previous weeks. 

Make an additive model that uses polynomial regression for each of the predictors.

```{r}
#| vscode: {languageId: r}
library(gam)
fit4 = gam(Volume ~ poly(Lag1,3 ) + poly(Lag2,3 ) + poly(Lag3,3 )) #polynomials of lag features
summary(fit4)
par(mfrow=c(1,3)) #to partition the Plotting Window
plot(fit4,se = TRUE)
```

Plot the data and the fits. 

```{r}
#| vscode: {languageId: r}
plot(Volume,col="darkgrey",type="l")
preds.Weekly=predict(fit4,se=TRUE)
lines(preds.Weekly$fit,lwd=2,col="blue") #need to treat this as time series data
```

Clearly the fits have nothing to do with the data. It is essential to include time in the fit. Also, the exponential increase of the trading volume suggests that one should look at the log of the volume.

```{r}
#| vscode: {languageId: r}
plot.ts(log10(Volume), type="l")
```

There is a general linear trend, the magnitude of the variation is about the same for all years, there are deviations from the linear trend around the years 2002 and 2008, and there may be seasonal variation. 


### Smoothing splines

 We can use smoothing splines to summarize these data. For very large penalty parameters $\lambda$ or equivalently $df = 2$, a smoothing spline is essentially a straight line. We'll make such a smoothing spline, compute predictions, and plot it. Choosing a large lambda is equivalent to using two degrees of freedom (two parameters are fitted).

```{r}
#| vscode: {languageId: r}
fit.s1 = smooth.spline(log10(Volume) ~  1:1089, df = 2) #constraint or flexible with df=2? linear?
preds.s1 = predict(fit.s1)
plot.ts(log10(Volume), col = "darkgrey")
lines(preds.s1$y, lwd =2 ) #high bias?low bias?
```

To capture year-to-year variation, increase the number of degrees of freedom. Let's use one df per year, plus one for the intercept. This is plotted in red.

```{r}
#| vscode: {languageId: r}
fit.s1 = smooth.spline(log10(Volume) ~  1:1089, df = 2)
preds.s1 = predict(fit.s1)
fit.s22 = smooth.spline(log10(Volume) ~  1:1089, df = 22)
preds.s22 = predict(fit.s22)

plot(log10(Volume), col = "darkgrey")
lines(preds.s1$y, lwd =2 )
lines(preds.s22$y, lwd =2, col = 2)
```

To capture also seasonal variation, increase the number of degrees of freedom further. We use four df per year, plus one for the intercept. This is plotted in blue.

```{r}
#| vscode: {languageId: r}
fit.s85 = smooth.spline(log10(Volume) ~  1:1089, df = 85)
preds.s85 = predict(fit.s85)

plot(log10(Volume), col = "darkgrey")
lines(preds.s85$y, lwd =2, col = 4) #what can you say about the overall market trend?
```

When the number of degrees of freedom is not specified, \texttt{gam} chooses one. Here is the resulting plot. This fit uses about 120 degrees of freedom.

```{r}
#| vscode: {languageId: r}
fit.s = smooth.spline(log10(Volume) ~  1:1089)
preds.s = predict(fit.s)

plot(log10(Volume), col = "darkgrey")
lines(preds.s$y, lwd =2, col = 4)
fit.s
```

### Solution: 

In this report, we analyzed the weekly S&P500 data and created models to predict the volume of trades through lagged values using polynomial regression and smoothing splines. The effects of Lag1, Lag2, and Lag3 on the trading volume in the polynomial regression model were found to be impactful because of their statistical significance. However, upon visualizing the fitted values, it was evident that the model trend was still inadequate, suggesting that adding more components, such as time, could help the situation.

More exploration with log transformation and smoothing splines showed that there seem to be changes in the volume of trades over time, with some deviation around strict market movements, such as in 2002 and 2008. The smoothing spline model with high degrees of freedom provided better estimates as it was able to model more of the short-term wiggles, which evidences the flexibility needed in financial data modeling.

In summary, our results indicate that while polynomial regression models appear to work, they must depend on time to accurately depict the movement of trade volumes. Further improvements could be made by using other economic variables or employing sophisticated time-series predictive models to improve their accuracy.


## Question-2
This question relates to the College data set.

(a) Split the data into a training set and a test set. Using out-of-state tuition as the response and the other variables as the predictors, perform forward stepwise selection on the training set in order to identify a satisfactory model that uses just a subset of the predictors. Plot the BIC to pick the best model (cross validation could also be used).

```{r}
# Load libraries
library(ISLR)    # Contains the College dataset
library(caret)   # For data splitting
library(leaps)   # For forward stepwise selection (BIC)
library(ggplot2) # For visualization

# Load the College dataset
data(College)

# Ensure the dataset is a data frame
College <- as.data.frame(College)

# Split data into 80% training and 30% test set
set.seed(100)
train_index <- createDataPartition(College$Outstate, p = 0.8, list = FALSE)
train_data <- College[train_index, ]
test_data <- College[-train_index, ]

# Perform forward stepwise selection using BIC
stepwise_model <- regsubsets(Outstate ~ ., data = train_data, nvmax = 10, method = "forward")

# Extract BIC values for each model size
bic_values <- summary(stepwise_model)$bic

# Plot BIC to determine the best model
plot(bic_values, type = "b", pch = 19, col = "blue",
     xlab = "Number of Predictors", ylab = "BIC",
     main = "BIC Values for Forward Stepwise Selection")

# Identify the optimal number of predictors (minimum BIC)
optimal_vars <- which.min(bic_values)
print(paste("Optimal number of predictors:", optimal_vars))

```

```{r}
print(colnames(train_data))
```

```{r}
print(str(train_data))  
print(dim(train_data)) 
print(head(train_data))  

```

(b) Fit a GAM on the training data, using out-of-state tuition as the response and the features selected in the previous step as the predictors. Plot the results, and explain your findings.

```{r}
# Load library
library(mgcv)  # For Generalized Additive Models

# Ensure column names are correctly formatted
colnames(train_data) <- make.names(colnames(train_data))

# Check if variables exist in the dataset
print(colnames(train_data))

# Ensure train_data is not empty
print(dim(train_data))  

# Define GAM formula with smoothing functions
formula <- Outstate ~ s(Room.Board, k = 4) + s(Enroll, k = 4) + s(Top10perc, k = 4)

# Fit a GAM model using selected predictors
gam_model <- gam(formula, data = train_data)

# Summary of the model
print(summary(gam_model))

# Plot the fitted smoothing functions
par(mfrow = c(1, 3))  # Arrange plots in one row
plot(gam_model, se = TRUE, col = "blue")

```

## GAM Model Analysis and Conclusions

1. Analysis of the Effect of Room. Board (Room and Board Fees) on Outstate Tuition
According to the GAM model, the relationship between Room. Board and Outstate tuition is almost perfectly linear as the cost of Room. Board increases, and so does the Outstate tuition. The estimated degrees of freedom suggest the variable has a linear effect (edf = 1.271), albeit with minor non-linear distortions. Moreover, this variable's impact on Outstate tuition is significant as shown by the extremely low p-value (p < 2e-16). 

2. Analysis of the Effect of Enroll (Number of Enrolled Students) on Outstate Tuition
The enrolling effect on Outstate tuition is apparent in its nonlinear behavior. The fit curve shows that capitation grants for smaller schools (lower enrollment values) have higher associated costs. With higher enrollment levels, tuition fees begin to decrease before leveling off. This may be the combination of having lower funds available in smaller private institutions and high pupil premium benefits in larger ones. The estimated degrees of freedom (edf = 2.166) support the claim of the variable having a non-linear effect, which is also true given the very low p-value.
3. Effect of Top10perc (Percentage of Freshmen from the Top 10% of Their High School Class) on Outstate Tuition
Top10perc variable has a strong non-linear relationship with Outstate tuition. The fitted smoothing curve shows that the increase in top-performing students directly correlates with increased tuition fees. This phenomenon indicates that schools with a more significant concentration of brilliant students tend to capture a better reputation and, hence, can impose higher fees. The estimated degrees of freedom (edf = 1.997) and the extremely small p-value prove the statistical significance of this relationship. 

4. Evaluation of the Model as a Whole 
The adjusted R-squared 0.578 suggests that the model captures about 57.8% of the variation in Outstate tuition. The deviance explained 58.2% shows that the predictors accounted well for the variance although other reasons could be causing the tuition fee. The smoothing terms indicate that non-linear factors are important when considering out-of-state tuition costs.



(c) Evaluate the model obtained on the test set, and explain the results obtained.

```{r}
# Predict on test data
test_predictions <- predict(gam_model, newdata = test_data)

# Calculate RMSE
rmse <- sqrt(mean((test_data$Outstate - test_predictions)^2))

# Calculate R-squared
ss_total <- sum((test_data$Outstate - mean(test_data$Outstate))^2)
ss_residual <- sum((test_data$Outstate - test_predictions)^2)
r_squared <- 1 - (ss_residual / ss_total)

# Print results
print(paste("Test RMSE:", round(rmse, 2)))
print(paste("Test R-squared:", round(r_squared, 4)))

```

1. Test RMSE = 2328.39
This RMSE value indicates that the model's predictions for out-of-state tuition error by 2328.39 on average. The performance is clearly low since the value is almost one at 2328.39, which is the RMSE deviation itself. From this value, it can be inferred that the average deviation of out-of-state tuition predictions made by the model from the actual values lies in a boundary of approximately 2328.39. A lower RMSE would mean a higher model performance, making the model incredible at making predictions.

2. Test R-squared = 0.6706
Out-of-state tuition, where the model captures variation that is explained on its own by the model, is a lower average of 67.06%. This indicates a moderately good fit for certain patterns, making it capture. However, it also states that the model does not capture complex non-linear patterns. It still captures 32.94% of the unexplained variation, which gives a sense the model is still trying to find patterns along with predictors.

(d) For which variables, if any, is there evidence of a non-linear relationship with the response? (make scatterplots of the response against the five numerical predictors.)

```{r}
# Load ggplot2 for visualization
library(ggplot2)

# Define selected numerical predictors
selected_vars <- c("Room.Board", "Enroll", "Top10perc", "F.Undergrad", "Expend")

# Create scatterplots with a smoothing curve using updated ggplot2 syntax
for (var in selected_vars) {
  print(ggplot(train_data, aes(x = .data[[var]], y = Outstate)) +
    geom_point(alpha = 0.5) +  # Scatterplot points
    geom_smooth(method = "loess", color = "blue") +  # Smooth curve to show trend
    labs(title = paste("Scatterplot of Outstate vs", var),
         x = var, y = "Outstate Tuition"))
}

```

The scatterplots demonstrate non-linear associations of out-of-state tuition (Outstate) with the four predictors: 

1.	Room. Board: Tuition grows with Room. Board, but the growth is lesser as the value increases, indicating a diminishing return effect.

2.	Enroll: An inverted U hypothesized pattern indicates that lesser-known colleges tend to charge higher tuition, while well-known colleges tend to have lower tuition.  

3.	F. Undergrad: Enroll trend where a lesser undergraduate population corresponds to higher tuition and a more significant population demonstrates the lessening effect.  

4.	Expend: Tuition grows with expenditure per student but flattens at the higher spending levels.  

All these relationships confirm the patterns and explain the primary reason, which is the non-linear modeling approach like GAMs.

